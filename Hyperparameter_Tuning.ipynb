{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e0d886",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "Trying to improve the results by using Bayesian Optimization for hyperparameter tuning.\n",
    "\n",
    "Bayesian Optimization is used to optimize the f1 in a first attempt, and in a second attempt to optimize the precision. Metrics and learning curves are used for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478606c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from src.data_prep_for_model import clean_data, feature_engineer, prep_data_for_model, pipeline_classifier\n",
    "\n",
    "# For Bayesian Optimization\n",
    "import time\n",
    "import optuna \n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# importing plotly and enable jupyter notebooks for showing optuna visualisations \n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0efcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/spotify_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae459a1",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6644408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features and target sets for train, test and val data from function output\n",
    "features_train, target_train, features_test, target_test, features_val, target_val = prep_data_for_model(df)\n",
    "\n",
    "features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e68e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting columns easy for copy-paste\n",
    "print(features_train.columns)\n",
    "\n",
    "# specific categories (for onehotencoding) and num cols list for pipeline\n",
    "CAT_COLS = ['key', 'time_signature']\n",
    "\n",
    "NUM_COLS = [col for col in features_train.columns if col not in CAT_COLS]\n",
    "\n",
    "print(CAT_COLS)\n",
    "print(NUM_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f9387",
   "metadata": {},
   "source": [
    "### Optimization on f1_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a4019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization on f1_weighed\n",
    "def objective(trial):\n",
    "    \"\"\"return maximized f1-score\"\"\"\n",
    "   \n",
    "    # search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 250)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "    max_features = trial.suggest_categorical('max_features', choices = ['sqrt', 'log2', None])\n",
    "    min_samples_split = trial.suggest_int(name=\"min_samples_split\", low=2, high=10, step=2)\n",
    "    min_samples_leaf = trial.suggest_int(name=\"min_samples_leaf\", low=1, high=4, step=1)\n",
    "    \n",
    "    params = {'n_estimators': n_estimators,\n",
    "             'max_features': max_features,\n",
    "             'max_depth': max_depth,\n",
    "             'min_samples_split': min_samples_split,\n",
    "             'min_samples_leaf': min_samples_leaf}\n",
    "    \n",
    "    # random forest classifier object     \n",
    "    pipeline = pipeline_classifier(cat_cols=CAT_COLS,\n",
    "                                    num_cols=NUM_COLS,\n",
    "                                    classifier=RandomForestClassifier,\n",
    "                                    class_weight='balanced',\n",
    "                                    random_state=42,\n",
    "                                    **params)\n",
    "\n",
    "    # initiating cv\n",
    "    score =  cross_val_score(estimator=pipeline, \n",
    "                             X=features_train, \n",
    "                             y=target_train, \n",
    "                             scoring='f1_weighted',\n",
    "                             cv=5,\n",
    "                             n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# create a study (aim to maximize score) und setting a seed (random_state) for reproduceability\n",
    "study = optuna.create_study(sampler=TPESampler(seed = 42), direction='maximize')\n",
    "\n",
    "# perform hyperparamter tuning (while timing the process)\n",
    "time_start = time.time()\n",
    "# starting optimization process with our defined function and 50 iterations\n",
    "study.optimize(objective, n_trials=50)\n",
    "time_bayesian = time.time() - time_start\n",
    "\n",
    "# store result in a data frame \n",
    "values_bayesian = [50, study.best_trial.number, study.best_trial.value, time_bayesian]\n",
    "results_bayesian = pd.DataFrame([values_bayesian], columns = ['Number of iterations', \n",
    "                                                                        'Iteration Number of Optimal Hyperparamters', \n",
    "                                                                        'Score', \n",
    "                                                                        'Time Elapsed (s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "display(results_bayesian)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model optimized on f1_weighted (manually filled from last optimization run, so the run does not need to be repeated (takes a while))\n",
    "best_params = {'n_estimators': 193, \n",
    "               'max_depth': 15,\n",
    "               'max_features': None,\n",
    "               'min_samples_split': 4,\n",
    "               'min_samples_leaf': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6803b17",
   "metadata": {},
   "source": [
    "### Optimization on precision score (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization on precision score weighted\n",
    "def objective(trial):\n",
    "    \"\"\"return maximized f1-score\"\"\"\n",
    "   \n",
    "    # search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 250)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 15)\n",
    "    max_features = trial.suggest_categorical('max_features', choices = ['sqrt', 'log2', None])\n",
    "    min_samples_split = trial.suggest_int(name=\"min_samples_split\", low=2, high=10, step=2)\n",
    "    min_samples_leaf = trial.suggest_int(name=\"min_samples_leaf\", low=1, high=4, step=1)\n",
    "    \n",
    "    params = {'n_estimators': n_estimators,\n",
    "             'max_features': max_features,\n",
    "             'max_depth': max_depth,\n",
    "             'min_samples_split': min_samples_split,\n",
    "             'min_samples_leaf': min_samples_leaf}\n",
    "    \n",
    "    # random forest classifier object     \n",
    "    pipeline = pipeline_classifier(cat_cols=CAT_COLS,\n",
    "                                    num_cols=NUM_COLS,\n",
    "                                    classifier=RandomForestClassifier,\n",
    "                                    class_weight='balanced',\n",
    "                                    random_state=42,\n",
    "                                    **params)\n",
    "    \n",
    "    # initiating cv\n",
    "    score =  cross_val_score(estimator=pipeline, \n",
    "                             X=features_train, \n",
    "                             y=target_train, \n",
    "                             scoring='precision_weighted',\n",
    "                             cv=5,\n",
    "                             n_jobs=-1).mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "# create a study (aim to maximize score) und setting a seed (random_state) for reproduceability\n",
    "study = optuna.create_study(sampler=TPESampler(seed = 42), direction='maximize')\n",
    "\n",
    "# perform hyperparamter tuning (while timing the process)\n",
    "time_start = time.time()\n",
    "# starting optimization process with our defined function and 50 iterations\n",
    "study.optimize(objective, n_trials=50)\n",
    "time_bayesian = time.time() - time_start\n",
    "\n",
    "# store result in a data frame \n",
    "values_bayesian = [50, study.best_trial.number, study.best_trial.value, time_bayesian]\n",
    "results_bayesian = pd.DataFrame([values_bayesian], columns = ['Number of iterations', \n",
    "                                                                        'Iteration Number of Optimal Hyperparamters', \n",
    "                                                                        'Score', \n",
    "                                                                        'Time Elapsed (s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc081c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "display(results_bayesian)\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c9c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model optimized on precision score weighted (manually filled from last optimization run, so the run does not need to be repeated (takes a while))\n",
    "best_params = {'n_estimators': 202, \n",
    "               'max_depth': 15,\n",
    "               'max_features': None,\n",
    "               'min_samples_split': 2,\n",
    "               'min_samples_leaf': 4}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
