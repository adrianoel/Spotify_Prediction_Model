{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fab174b",
   "metadata": {},
   "source": [
    "# Base model\n",
    "\n",
    "Final base model will be built and tested. Feature engineering is applied; numerical features are standardized.  \n",
    "No Hyperparameter-Tuning yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from src.features.data_prep_for_model import clean_data, feature_engineer, prep_data_for_model, pipeline_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0550f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('data/spotify_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f4d9e",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7769b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features and target sets for train, test and val data from function output\n",
    "features_train, target_train, features_test, target_test, features_val, target_val = prep_data_for_model(df)\n",
    "\n",
    "features_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting columns easy for copy-paste\n",
    "print(features_train.columns)\n",
    "\n",
    "# specific categories (for onehotencoding) and num cols list for pipeline\n",
    "CAT_COLS = ['key', 'time_signature']\n",
    "\n",
    "NUM_COLS = [col for col in features_train.columns if col not in CAT_COLS]\n",
    "\n",
    "print(CAT_COLS)\n",
    "print(NUM_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ebba97",
   "metadata": {},
   "source": [
    "### Model training\n",
    "Models of interest: Classifiers with \"balanced weight\" parameter like RandomForestClassifier and LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2077c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model #1: RandomForestClassifier\n",
    "# use pipeline function for preprocessing\n",
    "pipeline_rfc = pipeline_classifier(cat_cols=CAT_COLS,\n",
    "                                   num_cols=NUM_COLS,\n",
    "                                   classifier=RandomForestClassifier,\n",
    "                                   class_weight='balanced',\n",
    "                                   random_state=42)\n",
    "\n",
    "# train model and predict on test data\n",
    "pipeline_rfc.fit(features_train, target_train)\n",
    "target_test_pred = pipeline_rfc.predict(features_test)\n",
    "\n",
    "# show metrics\n",
    "print('Confusion Matrix: \\n', confusion_matrix(target_test, target_test_pred), '\\n')\n",
    "print('Classification Report: \\n', classification_report(target_test, target_test_pred))\n",
    "\n",
    "# predict on val data\n",
    "target_val_pred = pipeline_rfc.predict(features_val)\n",
    "\n",
    "# show metrics\n",
    "print('Confusion Matrix: \\n', confusion_matrix(target_val, target_val_pred), '\\n')\n",
    "print('Classification Report: \\n', classification_report(target_val, target_val_pred))\n",
    "\n",
    "# save classification report of val data in results folder of src to load it later for direct comparison\n",
    "rfc_model_classification_report = classification_report(target_val, target_val_pred, output_dict=True)\n",
    "rfc_model_classification_report = pd.DataFrame(rfc_model_classification_report).transpose()\n",
    "rfc_model_classification_report.columns = ['precision_rfc', 'recall_rfc', 'f1_score_rfc', 'support_rfc']\n",
    "rfc_model_classification_report.to_csv('classification_reports/rfc_model_classification_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c88d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model #2: Logistic Regression\n",
    "# use pipeline function for preprocessing\n",
    "pipeline_log = pipeline_classifier(cat_cols=CAT_COLS,\n",
    "                                   num_cols=NUM_COLS,\n",
    "                                   classifier=LogisticRegression,\n",
    "                                   max_iter=1000,\n",
    "                                   C=0.5,\n",
    "                                   class_weight='balanced',\n",
    "                                   random_state=42)\n",
    "\n",
    "# train model and predict on test data\n",
    "pipeline_log.fit(features_train, target_train)\n",
    "target_test_pred = pipeline_log.predict(features_test)\n",
    "\n",
    "# show metrics\n",
    "print('Confusion Matrix: \\n', confusion_matrix(target_test, target_test_pred), '\\n')\n",
    "print('Classification Report: \\n', classification_report(target_test, target_test_pred))\n",
    "\n",
    "# predict on val data\n",
    "target_val_pred = pipeline_log.predict(features_val)\n",
    "\n",
    "# show metrics\n",
    "print('Confusion Matrix: \\n', confusion_matrix(target_val, target_val_pred), '\\n')\n",
    "print('Classification Report: \\n', classification_report(target_val, target_val_pred))\n",
    "\n",
    "# save classification report of val data in results folder of src to load it later for direct comparison\n",
    "log_model_classification_report = classification_report(target_val, target_val_pred, output_dict=True)\n",
    "log_model_classification_report = pd.DataFrame(log_model_classification_report).transpose()\n",
    "log_model_classification_report.columns = ['precision_log', 'recall_log', 'f1_score_log', 'support_log']\n",
    "log_model_classification_report.to_csv('classification_reports/log_model_classification_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare classification reports of all models (skip last 2 rows of macro avg and weighted avg)\n",
    "simple_baseline_report = pd.read_csv('classification_reports/simple_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "rfc_model_report = pd.read_csv('classification_reports/rfc_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "log_model_report = pd.read_csv('classification_reports/log_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "\n",
    "reports_combined = pd.concat([log_model_report, simple_baseline_report, rfc_model_report], axis=1)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "reports_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cross validation score of model #1: RandomForestClassifier\n",
    "cv_results_rfc = cross_val_score(estimator=pipeline_rfc,\n",
    "                            X=features_train,\n",
    "                            y=target_train,\n",
    "                            cv=5,\n",
    "                            scoring='f1_weighted',\n",
    "                            n_jobs=-1)\n",
    "cv_results_rfc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b740fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check cross validation score of model #2: LogisticRegression\n",
    "cv_results_log = cross_val_score(estimator=pipeline_log,\n",
    "                            X=features_train,\n",
    "                            y=target_train,\n",
    "                            cv=5,\n",
    "                            scoring='f1_weighted',\n",
    "                            n_jobs=-1)\n",
    "cv_results_log.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628065f",
   "metadata": {},
   "source": [
    "### Model interpretation (only for favored random forest model, no hypertuning yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23340e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final model chosen based on previous scores: RandomForestClassifier\n",
    "# check feature_importances_\n",
    "\n",
    "# get the classifier and preprocessor\n",
    "model = pipeline_rfc.named_steps['classifier']\n",
    "preprocessor = pipeline_rfc.named_steps['preprocessor']\n",
    "\n",
    "# get feature names after ColumnTransformer\n",
    "num_features = preprocessor.transformers_[0][2]\n",
    "cat_features = preprocessor.transformers_[1][1].get_feature_names_out(preprocessor.transformers_[1][2])\n",
    "all_features = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# combine into a DataFrame\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# plot top N\n",
    "top_n = 20\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.barh(feature_importances.head(top_n).iloc[::-1]['feature'],\n",
    "         feature_importances.head(top_n).iloc[::-1]['importance'])\n",
    "ax.set_xlabel(\"Feature Importance\")\n",
    "ax.set_title(f\"Top {top_n} features of final model\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing learning curve (could take some time)\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=RandomForestClassifier(class_weight='balanced', random_state = 42), \n",
    "                                                        X=features_train, \n",
    "                                                        y=target_train, \n",
    "                                                        cv=5, \n",
    "                                                        scoring='f1_weighted',\n",
    "                                                        n_jobs=-1,\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "train_sizes_lc = train_sizes\n",
    "train_mean_lc = train_scores.mean(axis=1)\n",
    "test_mean_lc = test_scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_lc, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(train_sizes_lc, train_mean_lc, label=\"train\", color = 'red')\n",
    "ax.plot(train_sizes_lc, test_mean_lc, label=\"validation\", color = 'blue')\n",
    "\n",
    "ax.set_title(\"Learning Curve\")\n",
    "ax.set_xlabel(\"Training Set Size\")\n",
    "ax.set_ylabel(\"F1-Score (weighted)\")\n",
    "ax.legend(loc=\"best\")\n",
    "fig_lc;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6f73a",
   "metadata": {},
   "source": [
    "### Final Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model (from hypertuning on f1_weighted)\n",
    "best_params = {'n_estimators': 182, \n",
    "               'max_depth': 15,\n",
    "               'max_features': 'sqrt',\n",
    "               'min_samples_split': 8,\n",
    "               'min_samples_leaf': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed4add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating predictions on test data\n",
    "best_model = RandomForestClassifier(class_weight='balanced', random_state = 42, **best_params)\n",
    "\n",
    "best_model.fit(features_train, target_train)\n",
    "\n",
    "target_test_pred = best_model.predict(features_test)\n",
    "\n",
    "print('f1_weighted score: \\n', f1_score(target_test, target_test_pred, average='weighted'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c86992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluating predictions on val data\n",
    "best_model = RandomForestClassifier(class_weight='balanced', random_state = 42, **best_params)\n",
    "\n",
    "best_model.fit(features_train, target_train)\n",
    "\n",
    "target_val_pred = best_model.predict(features_val)\n",
    "\n",
    "print('f1_weighted score: \\n', f1_score(target_val, target_val_pred, average='weighted'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcde56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save classification report of val data in results folder of src to load it later for direct comparison\n",
    "rfc_best_model_classification_report = classification_report(target_val, target_val_pred, output_dict=True)\n",
    "rfc_best_model_classification_report = pd.DataFrame(rfc_model_classification_report).transpose()\n",
    "rfc_best_model_classification_report.columns = ['precision_rfc_best', 'recall_rfc_best', 'f1_score_rfc_best', 'support_rfc_best']\n",
    "rfc_best_model_classification_report.to_csv('classification_reports/rfc_best_model_classification_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare classification reports of all models (skip last 2 rows of macro avg and weighted avg)\n",
    "simple_baseline_report = pd.read_csv('classification_reports/simple_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "rfc_model_report = pd.read_csv('classification_reports/rfc_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "log_model_report = pd.read_csv('classification_reports/log_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "rfc_best_model_report = pd.read_csv('classification_reports/rfc__best_model_classification_report.csv', index_col=0, skiprows=[6, 7])\n",
    "\n",
    "reports_combined = pd.concat([log_model_report, simple_baseline_report, rfc_model_report], axis=1)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "reports_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aba306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature_importances_\n",
    "\n",
    "# get the classifier and preprocessor\n",
    "model = pipeline_rfc.named_steps['classifier']\n",
    "preprocessor = pipeline_rfc.named_steps['preprocessor']\n",
    "\n",
    "# get feature names after ColumnTransformer\n",
    "num_features = preprocessor.transformers_[0][2]\n",
    "cat_features = preprocessor.transformers_[1][1].get_feature_names_out(preprocessor.transformers_[1][2])\n",
    "all_features = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# combine into a DataFrame\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# plot top N\n",
    "top_n = 20\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.barh(feature_importances.head(top_n).iloc[::-1]['feature'],\n",
    "         feature_importances.head(top_n).iloc[::-1]['importance'])\n",
    "ax.set_xlabel(\"Feature Importance\")\n",
    "ax.set_title(f\"Top {top_n} features of final model\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb19129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing learning curve (could take some time)\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=RandomForestClassifier(class_weight='balanced', random_state = 42), \n",
    "                                                        X=features_train, \n",
    "                                                        y=target_train, \n",
    "                                                        cv=5, \n",
    "                                                        scoring='f1_weighted',\n",
    "                                                        n_jobs=-1,\n",
    "                                                        train_sizes=np.linspace(0.1, 1.0, 5))\n",
    "\n",
    "train_sizes_lc = train_sizes\n",
    "train_mean_lc = train_scores.mean(axis=1)\n",
    "test_mean_lc = test_scores.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd2af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_lc, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(train_sizes_lc, train_mean_lc, label=\"train\", color = 'red')\n",
    "ax.plot(train_sizes_lc, test_mean_lc, label=\"validation\", color = 'blue')\n",
    "\n",
    "ax.set_title(\"Learning Curve\")\n",
    "ax.set_xlabel(\"Training Set Size\")\n",
    "ax.set_ylabel(\"F1-Score (weighted)\")\n",
    "ax.legend(loc=\"best\")\n",
    "fig_lc;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
